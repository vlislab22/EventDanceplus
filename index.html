<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>EventDance</title>
    <meta name="author" content="Xu Zheng">
    <meta name="description" content="Project page of EventDance">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>

  </head>

<body style=“text-align: center;”>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition<br /> 
                <small>
                    Accepted to CVPR 2024
                </small>
            </h1>
        </div> 


        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
			<li>
			<img src="./images/xu1.png" height="80px"><br>
			<a href="https://zhengxujosh.github.io/" >
                        Xu Zheng*
                      </a>
                      <br /> AI Thrust, HKUST(GZ)
                      <br /> &nbsp &nbsp
                    </li>
		 <li>
			    <img src="./images/Addision.png" height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">
                           Addison Lin Wang
                        </a>
                        <br /> AI & CMA Thrust, HKUST(GZ) 
			    <br/> Dept. of CSE, HKUST 
                    </li>
		</ul>
            </div>
        </div>

        <!-- ##### Elements #####-->
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
			    <a href="">
                            <img src="./images/arxiv.png" height="100px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
			    <!-- <a href="">
                            <img src="./images/youtube_icon.jpg" height="100px"><br>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li> -->
                            <a href="https://github.com/QC-LY/EventDance">
                            <img src="./images/github_icon.jpg" height="100px"><br>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>

                        <!-- <li>
                            
                            <img src="./images/colab_icon.jpg" height="100px"><br>
                                <h4><strong>Colab</strong></h4>
                            </a>
                        </li> -->
 

<!--                         <li>
                            <a href="https://github.com/jiazhou-garland/ELIP/blob/master/Appendix.pdf">
                            <img src="./images/slide_icon.jpg" height="100px"><br>
                                <h4><strong>Supp</strong></h4>
                            </a>
                        </li>                      -->
                        <li>
                            <a href="https://vlislab22.github.io/vlislab/">
                            <img src="./images/lab_logo.png" height="100px"><br>
                                <h4><strong>Vlislab</strong></h4>
                            </a>
                        </li>                       
                      
                    </ul>
                </div>
        </div>

        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    In this paper, we make the \textbf{first} attempt at achieving the cross-modal (\ie, image-to-events) adaptation for event-based object recognition \textbf{without accessing} any labeled source image data owning to privacy and commercial issues. Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events. In particular, as only the source model is available, a hurdle is how to extract the knowledge from the source model by only using the unlabeled target event data while achieving knowledge transfer. To this end, we propose a novel framework, dubbed \textbf{EventDance} for this unsupervised source-free cross-modal adaptation problem. Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (\textbf{RMB}) module, which reconstructs intensity frames from events in a self-supervised manner. This makes it possible to build up the surrogate images to extract the knowledge (\ie, labels) from the source model. We then propose a multi-representation knowledge adaptation (\textbf{MKA}) module that transfers the knowledge to target models learning events with multiple representation types for fully exploring the spatiotemporal information of events. The two modules connecting the source and target models are mutually updated so as to achieve the best performance. Experiments on three benchmark datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data.
                </p>
            </div>
        </div>

 

        <!-- ##### Results #####-->

     <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Overall framework of our EventDance
            </h3>
		<p class="text-justify">
			An overview of our EventDance.
		</p>
            <img src="./images/overall.jpg" class="img-responsive" alt="vis_res"  class="center" >
      	</div>
    </div>

   <!-- ##### BibTex #####-->
        <hr>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
 
                <div class="row align-items-center">
                    <div class="col py-3">
                        <pre class="border">             
@article{,
  title={EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition},
  author={Zheng,Xu and Wang,Lin},
  journal={CVPR},
  year={2024}
}
</pre>
                    </div>
                </div>
              
    
          </div>
          
        </div>
    </div>
</body>
</html>
